{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/201508_trip_data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = (\n",
    "    spark.read.format(file_type)\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(file_location)\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view or table\n",
    "\n",
    "temp_table_name = \"/FileStore/tables/201508_trip_data.csv\"\n",
    "\n",
    "df.createOrReplaceTempView(\"trip_data\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    regexp_replace,\n",
    "    regexp_extract,\n",
    "    when,\n",
    "    array_contains,\n",
    "    avg,\n",
    "    sum,\n",
    "    countDistinct,\n",
    "    isnan,\n",
    "    split,\n",
    "    expr,\n",
    "    pandas_udf,\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69183ba-ebf7-42a2-ab3b-a804f4c4b44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ff8395-50e3-4f29-b9e3-a570b6c6a9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Użycie poniższych funkcji: Nulls, fill, explode, drop, regexp_replace, regexp_extract, ifnull, nullIf, replace, array_contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba097bb-69fc-4eaa-9d15-98d4bb164089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a.\tUżyj poniższe funkcje Nulls, fill, explode, drop, regexp_replace, regexp_extract, ifnull, nullIf, replace, array_contains.\n",
    "\n",
    "# Nulls - sprawdzanie, czy kolumna zawiera wartości NULL\n",
    "\n",
    "\n",
    "df.filter(col(\"Zip Code\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149e94a2-56a1-45ca-ba7c-35f59fc8f113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill - Wypełnienie wartości NULL w danej kolumnie:\n",
    "\n",
    "df.fillna({\"Zip Code\": \"Brak danych\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2affcba-5ba2-4226-95b5-5d8c47fa9231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# explode - jeśli mamy kolumne z listami i chcemy każdą wartość w osobnym wierszu\n",
    "df_exploded = df.withColumn(\"Exploded_Start\", explode(split(col(\"Start Date\"), \" \")))\n",
    "display(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c025d02-20c5-40fd-a778-fffa718f74b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a.\tUżyj poniższe funkcje Nulls, fill, explode, drop, regexp_replace, regexp_extract, ifnull, nullIf, replace, array_contains.\n",
    "\n",
    "# drop - sluzy do usuwania\n",
    "\n",
    "df_dropped = df.drop(\"Exploded_Start\")\n",
    "display(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b19044-68b5-4018-9215-bb68ab2bc557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# regexp_replace - zamienianie wzorcow znakow\n",
    "\n",
    "df_replaced = df.withColumn(\n",
    "    \"Start Station\",\n",
    "    regexp_replace(\n",
    "        df[\"Start Station\"], \"San Antonio Shopping Center\", \"SA Shopping Center\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(df_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99cf894-f69e-468e-9ed7-d73c7dec99e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# regexp_extract - pozwala wyciągnąć fragmenty tekstu pasujące do wyrażenia regularnego.\n",
    "from pyspark.sql.functions import regexp_extract, col, coalesce, lit\n",
    "\n",
    "df_reg = df.withColumn(\n",
    "    \"Extracted\", regexp_extract(col(\"Start Station\"), r\"\\bS\\w*\\b\", 0)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Start Station\", coalesce(col(\"Start Station\"), lit(\"Unknown Station\"))\n",
    ")\n",
    "df_reg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7848b3a8-0cc1-4b26-91c8-98017bf98505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ifnull - jesli wartość w danej kolumnie jest NULL, zamienia ją na określoną wartośc\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "df_null = df.withColumn(\n",
    "    \"Start Station\", F.coalesce(F.col(\"Start Station\"), F.lit(\"Unknown Station\"))\n",
    ")\n",
    "display(df_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66275ad7-a238-4dd1-b40f-ec4b6141a845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  nullIf - porównuje dwie wartości i zwraca null, jeśli te wartości są równe, w przeciwnym razie zwraca pierwszą wartość\n",
    "\n",
    "df_nullif = df.withColumn(\"Duration\", expr(\"nullIf(Duration, '444')\"))\n",
    "display(df_nullif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08929829-1862-458a-a0aa-79a09f558f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  replace - służy do zastępowania określonych wartości w DataFrame na inne\n",
    "df_replaced = df.replace(\n",
    "    \"Spear at Folsom\", \"Commercial at Montgomery\", subset=[\"Start Station\"]\n",
    ")\n",
    "display(df_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee2abc0-3fa3-43c9-aef8-bd01a95c7524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# array_contains - służy do sprawdzania, czy w kolumnie zawierającej tablicę (array) znajduje się określona wartość\n",
    "\n",
    "df_replaced = df_replaced.withColumn(\"end_array\", split(col(\"End Station\"), \" \"))\n",
    "df_replaced = df_replaced.withColumn(\n",
    "    \"contains_San_Francisco\", array_contains(col(\"end_array\"), \"San Francisco\")\n",
    ")\n",
    "\n",
    "\n",
    "display(df_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ec7cb4-2a9a-4702-a932-ca72710a75f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Użycie 3 funkcji agregujących "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa40903-3294-41c8-b99e-eca8609bae9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, min, max\n",
    "\n",
    "# 1 avg\n",
    "df_avg_dur = df.groupBy(\"Start Station\").agg(avg(\"Duration\").alias(\"avg_dur\"))\n",
    "display(df_avg_dur)\n",
    "\n",
    "# 2  count\n",
    "df2 = df.groupBy(\"Bike #\").agg(count(\"Bike #\").alias(\"bike_count\"))\n",
    "display(df2)\n",
    "\n",
    "# 3 - sum\n",
    "df_sum_dur = df.groupBy(\"Start Station\").agg(sum(\"Duration\").alias(\"total_dur\"))\n",
    "display(df_sum_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e223c1c-ce32-48e8-97d5-94a5671ed2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813825e7-7794-4a02-8c20-82eae5a9e0b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funkcje UDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26997059-4897-43b3-8c2c-16428b15622e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# funkcja do obliczania podatku\n",
    "df_test = df.withColumn(\"Bike #\", col(\"Bike #\").cast(\"int\"))\n",
    "\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def calculate_tax(dockcount: pd.Series) -> pd.Series:\n",
    "    dockcount = pd.to_numeric(\n",
    "        dockcount, errors=\"coerce\"\n",
    "    )  # Zmieniamy teksty na NaN, jeżeli nie uda się konwertować\n",
    "    tax = dockcount.apply(lambda x: x * 0.05 if x > 20 else 2 if pd.notna(x) else 0)\n",
    "    return tax\n",
    "\n",
    "\n",
    "df_with_tax = df_test.withColumn(\"tax\", calculate_tax(df_test[\"Bike #\"]))\n",
    "display(df_with_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f4f6bb-4007-45e6-aae8-9618f656209f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# funkcja która sprawdza, czy w 'Start Station' znajduje się słowo 'San'\n",
    "def contains_san(start_station: str) -> bool:\n",
    "    if start_station and \"San\" in start_station:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Rejestracja funkcji jako UDF\n",
    "contains_san_udf = udf(contains_san, BooleanType())\n",
    "\n",
    "# Dodanie nowej kolumny do DataFrame\n",
    "df_with_san_flag = df.withColumn(\"contains_san\", contains_san_udf(df[\"Start Station\"]))\n",
    "\n",
    "# Wyświetlenie wyniku\n",
    "display(df_with_san_flag)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab4_1-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}